# MCP

## MCP protocol

MCP este un protocol deschis care standardizeazÄƒ modul Ã®n care aplicaÈ›iile oferÄƒ context modelelor lingvistice mari (LLM). MCP este asemenea unui port USB-C pentru aplicaÈ›ii de inteligenÈ›Äƒ artificialÄƒ. AÈ™a cum USB-C oferÄƒ o modalitate standardizatÄƒ de a conecta dispozitivele la diverse periferice È™i accesorii, MCP oferÄƒ o modalitate standardizatÄƒ de a conecta modelele de inteligenÈ›Äƒ artificialÄƒ la diferite surse de date È™i instrumente. MCP permite construcÈ›ia de agenÈ›i È™i fluxuri de lucru complexe pe baza LLM-urilor È™i conectarea modelele cu lumea.

MCP oferÄƒ:

- O listÄƒ mare de integrÄƒri predefinite la care LLM-ul se poate conecta direct.
- O modalitate standard pentru a construi integrÄƒri personalizate pentru aplicaÈ›iile AI.
- Un protocol deschis pe care toatÄƒ lumea Ã®l poate implementa È™i folosi.
- Flexibilitatea de a schimba Ã®ntre diferite aplicaÈ›ii È™i de a lua contextul pentru utilizare.

MCP constÄƒ Ã®n douÄƒ layere:

- Data Layer: defineÈ™te protocolul bazat pe JSON-RPC pentru comunicarea client-server, incluzÃ¢nd managementul ciclului de viaÈ›Äƒ, primitive esenÈ›iale, cum ar fi instrumente, resurse, prompt-uri È™i notificÄƒri.
- Transport Layer: defineÈ™te mecanismul de comunicare È™i canalele care permit schimbul de date dintre client È™i server, incluzÃ¢nd È™i stabilizarea conexiunii specifice transportului, Ã®nrÄƒmarea mesajelor È™i autorizare.

## Arhitectura Proiectului

### Schema Bloc Software

![Schema_Bloc](Schema_Bloc.svg)

### ConsideraÈ›ii teoretice de implementare utilizate

Comunicarea dintre server È™i client se poate efectua prin fluxuri standard (STDIO) È™i prin protocolul HTTP.

Ãn aceastÄƒ implementare a fost aleasÄƒ o abordare bazatÄƒ pe protocolul HTTP Ã®ntrucÃ¢t cel mai adesea, pentru mulÈ›i utilizatori, serverul È™i clientul nu le sunt ambele accesibile, aceÈ™tia putÃ¢nd avea la dispoziÈ›ie doar una din cele douÄƒ componente care opereazÄƒ local.
Cu toate acestea a fost studiatÄƒ È™i o implementare prin fluxuri standard.

| HTTP | STDIO |
| ---------- | --------- |
| Serverul poate rula pe altÄƒ maÈ™inÄƒ Ã®n cloud, containerizat | Comunicare Ã®n cadrul aceluiaÈ™i proces |
| Integrare mai uÈ™oarÄƒ cu OpenWebUI sau cu alte aplicaÈ›ii care folosesc HTTP | Izolare mai bunÄƒ, ceea ce creÈ™te securitatea |
| Testare È™i Debugging mai simple | PerformanÈ›Äƒ bunÄƒ(nu existÄƒ overhead) |
| Scalabilitate, se poate adauga autentificare, logging centralizat | Bun pentru testare È™i integrare Ã®n sisteme CLI |

#### Serverul

Protocolul MCP presupune existenÈ›a a douÄƒ entitÄƒÈ›i: serverul, care expune instrumente, È™i clientul, care le consumÄƒ prin intermediul unui LLM.

Serverul este puntea de legÄƒturÄƒ dintre sistemul de gestiune al fiÈ™ierelor (sau o altÄƒ construcÈ›ie cu totul) È™i client. El trebuie sÄƒ expunÄƒ instrumentele utilizabile cÄƒtre client astfel Ã®ncÃ¢t llm-ul sÄƒ poatÄƒ È™ti ce are la dispoziÈ›ie pentru a Ã®ndeplini cerinÈ›ele user-ului.

Tool-urile sunt definite conform specificaÈ›iei MCP, fiecare avÃ¢nd un nume, o descriere È™i un schelet JSON-Schema pentru parametri:

(exemplu este implementarea tool-ului list_root_children)
```
Tool(
  name="list_root_children",
  description="ListeazÄƒ fiÈ™ierele È™i folderele din root-ul Alfresco",
  inputSchema={
    "type": "object",
    "properties": {
    "maxItems": {
        "type": "integer",
        "description": "NumÄƒrul maxim de elemente de returnat (default: 20)",
        "default": 20
      }
    }
  }
)
```

Serverul are trei responsabilitÄƒÈ›i principale:
- expune tool-urile disponibile cÄƒtre client.
- gestioneazÄƒ apelurile de la client È™i returneazÄƒ rÄƒspunsurile Ã®n format MCP.
- asigurÄƒ conectarea la sursa de date(Ã®n acest caz, Alfresco).

#### Clientul

Clientul asigurÄƒ conexiunea dintre utilizator È™i LLM, astfel cÄƒ utilizatorul poate vedea Ã®n timp real rezultatele furnizate de LLM È™i Ã®n funcÈ›ie de aceste rezultate poate veni cu alte cerinÈ›e mai complexe care Ã®n cele din urmÄƒ se vor definitiva Ã®n ceea ce voia utilizatorul sÄƒ facÄƒ.

De asemenea, system prompt-ul È™i function calling-ul se regÄƒsesc aici, deoarece, Ã®n client, acestea sunt cÃ¢t mai apropiate de LLM, ele find destinate cu precÄƒdere acestuia. Acestea au rolul de servi scopului final pentru care utilizatorul vrea sÄƒ foloseascÄƒ LLM-ul.

System prompt-ul are definit rolul Ã®n care LLM-ul trebuie sÄƒ se regÄƒseascÄƒ pentru a duce la Ã®ndeplinire sarcinile. Acesta conÈ›ine È™i lista de tool-uri disponibile È™i modalitatea Ã®n care rÄƒspunsul trebuie sÄƒ fie formulat pentru utilizator:

```
system_prompt = f"""=== CONTEXT SYSTEM ===
        
    EÈ™ti un asistent AI expert Ã®n Alfresco Document Management System cu acces la tool-uri MCP prin HTTP.

    Server MCP: {self.mcp_server_url}
    Model: {self.provider.upper()} - {self.model}
    {connection_info}

    Ai aceste informaÈ›ii despre tool-urile disponibile È™i modalitatea Ã®n care sunt ele folosite:
    {tools_info}

    Acesta este contextul curent:
    {context_str}

    Acestea sunt rezultatele anterioare returnate de tool-uri
    {tool_results_str}

    === INSTRUCÈšIUNI ===
    - DacÄƒ ai rezultate de la tool-uri MCP, foloseÈ™te-le Ã®n rÄƒspuns
    - RespectÄƒ schema parametrilor din tool-uri; tipurile numerice trebuie sÄƒ fie integer.
    - DacÄƒ tool-ul nu este disponibil sau eÈ™ueazÄƒ, explicÄƒ utilizatorului.
    - FoloseÈ™te contextul È™i rezultatele anterioare pentru a explica acÈ›iunile.
    - FoloseÈ™te tool-ul care se potriveÈ™te cÃ¢t mai bine cu cererea.
    - RÄƒspunde concis È™i profesional, fÄƒrÄƒ a repeta inutil contextul.
    - GenereazÄƒ doar rÄƒspunsuri utile pentru cererea curentÄƒ, evitÃ¢nd textul suplimentar.
    - Nu Ã®ncerca sÄƒ apelezi tool-uri direct din acest prompt â€” asta se face separat prin analiza intenÈ›iei.

    === CERERE UTILIZATOR ===
    {user_input}

    === RÄ‚SPUNS ===
    """

        return system_prompt
```

Function calling-ul este utilizat pentru a-i furniza LLM-ului informaÈ›ii despre cum trebuie sÄƒ apeleze tool-urile pe care le are la dispoziÈ›ie. AdicÄƒ trebuie sÄƒ extrag din input-ul user-ului argumentele pentru a apela acele tool-uri care se potrivesc cu cerinÈ›ele user-ului:

```
if action == "call_tool":
  tool_name = analysis.get('tool_name')
  arguments = analysis.get('arguments', {})
  explanation = analysis.get('explanation', '')

  if tool_name not in self.mcp_tools:
    return f"âŒ Tool-ul '{tool_name}' nu este disponibil"

  print(f"ğŸ¯ Execut: {explanation}")
  tool_result = await self.call_mcp_tool_http(tool_name, arguments)
  formatted = self.format_tool_result(tool_name, tool_result)
  return f"ğŸ“ ExplicaÈ›ie: {explanation}\n\nğŸ”§ Rezultat {tool_name}:\n{formatted}"
```

Clientul nu implementeazÄƒ logica tool-urilor, ci doar apeleazÄƒ tool-urile expuse de server prin MCP È™i prezintÄƒ rezultatele utilizatorului.

#### DiagramÄƒ de flux

#### OpenWebUI

OpenWebUI a fost folosit pentru a oferi o interfata grafica Ã®n cadrul cÄƒreia sÄƒ se expunÄƒ Ã®ntr-un mod mai simplu de vizualizat pentru utilizator rÄƒspunsurile la cererile sale.

Rezultatele furnizate de cÄƒtre llm sunt trimise de cÄƒtre client la interfaÈ›a OpenWebUI cu ajutorul protocolului http.

Ãn cadrul OpenWebUI se gÄƒseÈ™te un model de ai care nu este nimic altceva decÃ¢t LLM-ul menÈ›ionat anterior doar cÄƒ a fost creat un endpoint de legÄƒturÄƒ pentru a da impresia cÄƒ acesta are acces direct la interfaÈ›Äƒ. Ãn cadrul acelui endpoint sunt expuse numele noului model È™i alte informaÈ›ii printre care È™i numele tool-urile expuse de server.

## Instalare È™i Configurare

Se deschide un virtual environment pentru python 3.12 Ã®n care se vor instala utilizÃ¢nd comanda `pip install --r requiremnets.txt` utilitarele necesare pentru rularea fiÈ™ierelor.

## Ghid de utilizare

1. Se pornesc containerele docker pentru Alfresco folosind ` docker compose up -d`.

2. Se porneÈ™te serverul MCP: `./alfresco_mcp_server.py`

3. Se porneÈ™te adapter.py care permite deschiderea OpenWebUI È™i a clientului. `python3 adapter.py`

4. Se porneÈ™te imaginea de docker pentru OpenWebUI:

```
docker run -d   
  -p 3000:3000   
  -v openwebui-data:/app/backend/data   
  -e PORT=3000   
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8001/v1   
  -e OPENAI_API_KEY=sk-anything   
  -e WEBUI_SECRET_KEY=your-secret-key   
  --add-host=host.docker.internal:host-gateway   
  --name openwebui   
  ghcr.io/open-webui/open-webui:main
```

OpenWebUI se deschide la http://localhost:3000. Ãn cazul Ã®n care se lucreazÄƒ cu port fowarding atunci localhost se schimbÄƒ Ã®ntr-o adresÄƒ ip.

## Testare

## Test prompts

Exemplele de testare au fost mutate Ã®n fiÈ™ierul Test-prompts.md pentru a nu Ã®ncÄƒrca fiÈ™ierul de documentaÈ›ie.

## Resurse


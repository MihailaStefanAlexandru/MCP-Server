import asyncio
import os
import sys
import time
import threading
from typing import Dict, Any, List, Optional
from pydantic import BaseModel
import logging

# Import clientul LLM existent
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from universal_llm_client import MCPStdioLLMClient

# Configurare logging mai detaliatƒÉ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    stream: bool = False
    max_tokens: Optional[int] = None
    temperature: Optional[float] = 0.7

class OpenWebUIAdapter:
    """
    Adapter corrigat pentru OpenWebUI
    """
    
    def __init__(self, config_path: str = "llm_config.json"):
        self.config_path = config_path
        self.llm_client = None
        self.client_lock = threading.Lock()
        self.last_activity = time.time()
        self.session_timeout = 1800  # 30 minute
        self.is_healthy = False
        
        logger.info("üîß OpenWebUI Adapter ini»õializat")
        
        # TesteazƒÉ configura»õia la ini»õializare
        self._test_configuration()

    def _test_configuration(self):
        """TesteazƒÉ configura»õia »ôi conexiunea la pornire"""
        try:
            # VerificƒÉ dacƒÉ fi»ôierul de configurare existƒÉ
            if not os.path.exists(self.config_path):
                logger.error(f"‚ùå Fi»ôierul de configurare {self.config_path} nu existƒÉ!")
                return
            
            # √éncearcƒÉ sƒÉ creeze un client de test
            test_client = self.get_or_create_llm_client()
            if test_client:
                logger.info("‚úÖ Configura»õia este validƒÉ")
                self.is_healthy = True
            else:
                logger.error("‚ùå Nu s-a putut crea clientul LLM")
                
        except Exception as e:
            logger.error(f"‚ùå Eroare la testarea configura»õiei: {e}")

    def get_or_create_llm_client(self) -> MCPStdioLLMClient:
        """
        Ob»õine clientul LLM cu logging √ÆmbunƒÉtƒÉ»õit
        """
        with self.client_lock:
            current_time = time.time()
            
            # VerificƒÉ dacƒÉ clientul existƒÉ »ôi este √ÆncƒÉ valid
            if self.llm_client is None or (current_time - self.last_activity > self.session_timeout):
                if self.llm_client is not None:
                    logger.info("üîÑ Recreez clientul LLM (session timeout)")
                    try:
                        self.llm_client.stop_mcp_server()
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Eroare la oprirea clientului vechi: {e}")
                
                logger.info(f"üöÄ Creez nou client LLM cu config: {self.config_path}")
                
                try:
                    self.llm_client = MCPStdioLLMClient(self.config_path)
                    
                    # Porne»ôte serverul MCP
                    if self.llm_client.start_mcp_server():
                        logger.info("‚úÖ Client LLM cu MCP creat cu succes")
                        self.is_healthy = True
                    else:
                        logger.warning("‚ö†Ô∏è Client LLM creat dar fƒÉrƒÉ conexiune MCP")
                        self.is_healthy = False
                        
                except Exception as e:
                    logger.error(f"‚ùå Eroare crearea clientului LLM: {e}")
                    self.is_healthy = False
                    raise
            
            self.last_activity = current_time
            return self.llm_client

    async def process_chat_async(self, messages: List[ChatMessage]) -> str:
        """
        ProceseazƒÉ chat-ul cu debugging √ÆmbunƒÉtƒÉ»õit »ôi error handling
        """
        def run_sync_chat():
            try:
                logger.info("üîÑ √éncep procesarea cererii...")
                
                # Ob»õine clientul LLM
                client = self.get_or_create_llm_client()
                
                if not client:
                    error_msg = "‚ùå Nu s-a putut ini»õializa clientul LLM. VerificƒÉ configura»õia."
                    logger.error(error_msg)
                    return error_msg
                
                # Extrage ultima cerere a utilizatorului
                user_message = messages[-1].content if messages else ""
                
                if not user_message.strip():
                    return "‚ÑπÔ∏è Te rog sƒÉ √Æmi spui ce vrei sƒÉ fac."
                
                logger.info(f"üìù Procesez mesaj de la user: {user_message[:100]}...")
                
                # VerificƒÉ conexiunea MCP
                if client.mcp_connected and client.mcp_tools:
                    logger.info(f"üîó MCP conectat cu {len(client.mcp_tools)} tools disponibile")
                    
                    # AnalizeazƒÉ »ôi apeleazƒÉ tool-uri MCP
                    logger.info("üõ†Ô∏è Apelez tools MCP...")
                    tool_results = client.analyze_intent_and_call_tools(user_message)
                    logger.info(f"‚úÖ Tool results: {len(tool_results) if tool_results else 0} rezultate")
                    
                    # CreeazƒÉ context pentru rƒÉspunsul final
                    context_messages = []
                    if len(messages) > 1:
                        recent_messages = messages[-3:-1] if len(messages) > 3 else messages[:-1]
                        for msg in recent_messages:
                            context_messages.append(f"{msg.role.title()}: {msg.content}")
                    
                    context_str = ""
                    if context_messages:
                        context_str = "\nContext conversa»õie:\n" + "\n".join(context_messages) + "\n"
                    
                    # CreeazƒÉ prompt √ÆmbunƒÉtƒÉ»õit
                    logger.info("üìã Creez prompt √ÆmbunƒÉtƒÉ»õit...")
                    enhanced_prompt = client.create_enhanced_prompt(user_message, tool_results)
                    
                    if context_str:
                        enhanced_prompt = enhanced_prompt.replace(
                            f"√éntrebarea curentƒÉ: {user_message}", 
                            f"{context_str}√éntrebarea curentƒÉ: {user_message}"
                        )
                    
                    logger.info("ü§ñ Trimit prompt cƒÉtre LLM...")
                    logger.debug(f"Prompt complet: {enhanced_prompt[:200]}...")
                    
                    # Ob»õine rƒÉspunsul de la LLM
                    response = client.query_llm_with_retry(enhanced_prompt, max_tokens=500)
                    
                    if response:
                        logger.info(f"‚úÖ RƒÉspuns primit de la LLM: {len(response)} caractere")
                        logger.debug(f"RƒÉspuns: {response[:200]}...")
                        return response
                    else:
                        error_msg = "‚ùå Nu am primit rƒÉspuns de la LLM."
                        logger.error(error_msg)
                        return error_msg
                        
                else:
                    logger.warning("‚ö†Ô∏è MCP nu este conectat - folosesc fallback")
                    
                    # Fallback dacƒÉ MCP nu este conectat
                    fallback_prompt = f"""E»ôti un asistent AI pentru Alfresco Document Management System.

Utilizatorul √ÆntreabƒÉ: {user_message}

Din pƒÉcate, nu am acces la tool-urile MCP Alfresco √Æn acest moment. 
Te rog sƒÉ explici ce ai fi vrut sƒÉ faci »ôi √ÆncearcƒÉ din nou mai t√¢rziu.

Pentru opera»õiuni Alfresco disponibile de obicei:
- Listarea fi»ôierelor »ôi folderelor  
- Crearea de foldere noi
- CƒÉutarea √Æn documente  
- Ob»õinerea de informa»õii despre noduri

RƒÉspuns:"""
                    
                    logger.info("ü§ñ Folosesc fallback prompt...")
                    response = client.query_llm_with_retry(fallback_prompt, max_tokens=300)
                    
                    if response:
                        logger.info(f"‚úÖ RƒÉspuns fallback primit: {len(response)} caractere")
                        return response
                    else:
                        error_msg = "‚ùå Nu am primit rƒÉspuns nici cu fallback."
                        logger.error(error_msg)
                        return error_msg
                    
            except Exception as e:
                error_msg = f"‚ùå Eroare √Æn procesarea chat-ului: {str(e)}"
                logger.error(error_msg, exc_info=True)
                return error_msg
        
        # RuleazƒÉ √Æn thread separar pentru a nu bloca FastAPI
        logger.info("üîÑ Lansez procesarea √Æn executor...")
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, run_sync_chat)
        logger.info(f"üì§ Rezultat final: {len(result) if result else 0} caractere")
        return result

    async def get_available_tools(self) -> List[Dict[str, Any]]:
        """Ob»õine lista tool-urilor cu error handling √ÆmbunƒÉtƒÉ»õit"""
        def get_tools_sync():
            try:
                client = self.get_or_create_llm_client()
                if not client:
                    logger.warning("‚ö†Ô∏è Nu s-a putut ob»õine clientul LLM pentru tools")
                    return []
                    
                if client.mcp_connected and client.mcp_tools:
                    tools_info = []
                    for tool_name, tool_data in client.mcp_tools.items():
                        tools_info.append({
                            "name": tool_name,
                            "description": tool_data.get("description", "FƒÉrƒÉ descriere"),
                            "parameters": list(tool_data.get("inputSchema", {}).get("properties", {}).keys())
                        })
                    logger.info(f"üõ†Ô∏è Returnez {len(tools_info)} tools")
                    return tools_info
                else:
                    logger.warning("‚ö†Ô∏è MCP nu este conectat sau nu sunt tools disponibile")
                    return []
            except Exception as e:
                logger.error(f"‚ùå Eroare ob»õinere tool-uri: {e}")
                return []
        
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, get_tools_sync)
        return result

    async def get_mcp_status(self) -> Dict[str, Any]:
        """Status √ÆmbunƒÉtƒÉ»õit cu mai multe informa»õii"""
        def get_status_sync():
            try:
                client = self.get_or_create_llm_client()
                if not client:
                    return {
                        "connected": False,
                        "error": "Nu s-a putut crea clientul LLM",
                        "tools_count": 0,
                        "resources_count": 0,
                        "healthy": False
                    }
                    
                status = {
                    "connected": client.mcp_connected,
                    "tools_count": len(client.mcp_tools),
                    "resources_count": len(client.mcp_resources),
                    "tools": list(client.mcp_tools.keys()),
                    "process_running": client.mcp_process is not None and client.mcp_process.poll() is None,
                    "provider": getattr(client, 'provider', 'unknown'),
                    "model": getattr(client, 'model', 'unknown'),
                    "last_activity": self.last_activity,
                    "config_path": self.config_path,
                    "healthy": self.is_healthy,
                    "session_timeout": self.session_timeout
                }
                
                return status
            except Exception as e:
                logger.error(f"‚ùå Eroare ob»õinere status: {e}")
                return {
                    "connected": False,
                    "error": str(e),
                    "tools_count": 0,
                    "resources_count": 0,
                    "healthy": False
                }
        
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, get_status_sync)
        return result

    def cleanup(self):
        """CurƒÉ»õƒÉ resursele"""
        with self.client_lock:
            if self.llm_client:
                try:
                    self.llm_client.stop_mcp_server()
                    logger.info("üßπ Resurse LLM client curƒÉ»õate")
                except Exception as e:
                    logger.error(f"‚ö†Ô∏è Eroare cleanup: {e}")
                finally:
                    self.llm_client = None
                    self.is_healthy = False
